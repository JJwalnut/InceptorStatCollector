# 数据库连接配置
jdbc.url=jdbc:hive2://your_host:10000/default;transaction.type=inceptor;hive.server2.idle.session.timeout=64800000;hive.server2.idle.operation.timeout=3600000
jdbc.driver=org.apache.hive.jdbc.HiveDriver
jdbc.username=your_username
jdbc.password=your_password

#====================== 目标表配置 ======================
#用于存储统计信息的目标表名称，格式：数据库名.表名
#- 若该表不存在，程序将自动创建
#- 表结构会根据 analyze.level 配置项的取值不同而有所差异
#- 格式示例：数据库名。表名（如 default.inceptor_stats_result_table）
#- 程序运行前，对应的数据库必须已存在
#- 该表采用 ORC 格式存储，且支持事务功能
#- 表按 data_day 字段进行分区（分区粒度为月度）
#- 表按 id 字段分桶，共分为 11 个桶，以提升查询性能
#- 可针对不同环境（开发、测试、生产）配置不同的表名
#- 示例：prod.inceptor_stats_result_table、test.inceptor_stats_result_table
inceptor.target.table.name=default.inceptor_stats_result_table

# 并发配置：并发执行 ANALYZE 的工作线程数
# - 值越大处理越快，但数据库压力越大
# - 建议范围：5-20，默认：10
# - 高性能数据库：15-20，共享数据库：5-10
concurrency.level=10

# 批量配置：批量写入的记录数
# - 值越大写入性能越好，但内存占用越大
# - 建议范围：200-1000，默认：500
# - 高数据量场景：500-1000，低内存环境：200-500
batch.size=500

# 结果队列容量：存储待写入统计结果的缓冲区大小
# - 建议为 batch.size 的 4-10 倍
# - 公式：result.queue.capacity ≥ batch.size × 4
# - batch.size=500 时，建议：2000-5000
result.queue.capacity=2000

# 分页配置：每次查询的表数量
# - 值越大查询次数越少，但内存占用越大
# - 建议范围：500-2000，默认：1000
# - 表数量多时用 1000-2000，表数量少时用 500-1000
page.size=1000

# 任务队列容量：存储待执行 ANALYZE 命令的队列大小
# - 必须大于等于总表数量
# - 建议为总表数量的 1.5-2 倍
# - 表数量 <10000：50000-100000，表数量 >10000：100000+
task.queue.capacity=100000

# 超时配置
#====================== 超时配置 ======================
#等待所有分析任务完成的超时时间（单位：小时）
#- 控制程序等待所有 ANALYZE 命令执行完毕的时长
#- 若达到超时时间，程序会记录一条警告日志，但继续执行后续流程
#- 建议配置范围：1-24 小时，默认值：2 小时
#- 适用于数据表数量 < 1000 的系统：1-2 小时
#- 适用于数据表数量 1000-10000 的系统：2-4 小时
#- 适用于数据表数量 > 10000 的系统：4-8 小时或更长
#- 配置为 0 或负数时，程序会无限等待（不推荐此配置）
#- 应根据数据表总数及单表复杂度调整该参数

analysis.task.timeout.hours=5

#等待批量写入线程完成的超时时间（单位：分钟）
#- 控制程序等待批量写入器完成所有数据写入的时长
#- 若达到超时时间，程序会记录一条警告日志，但继续执行后续流程
#- 建议配置范围：10-120 分钟，默认值：30 分钟
#- 适用于小数据量场景：15-30 分钟
#- 适用于中等数据量场景：30-60 分钟
#- 适用于大数据量场景：60-120 分钟
#- 该参数值应小于 analysis.task.timeout.hours 对应的分钟数
#- 示例：若 analysis.task.timeout.hours = 2，则此参数值应小于 120 分钟
batch.writer.timeout.minutes=30

# 统计级别配置
# partition: 统计表级别和所有分区级别的统计信息（默认）
# table: 只统计表级别的汇总统计信息（不包含分区信息）
analyze.level=table


#从 system.tables_v 中获取数据表列表的 SQL 查询语句
#- 该 SQL 用于查询所有需要采集统计信息的数据表
#- 必须返回单列结果，且列中数据表名称格式为：数据库名.表名
#- 程序会自动为每一张匹配的数据表生成对应的 ANALYZE 命令
#- 可自定义 WHERE 子句，用于排除特定类型的数据表
#- 常见的排除类型：hyperbase 表、scope 表、hyperdrive 表等
#- 也可通过在 NOT IN 子句中添加数据库名，排除指定的数据库
#- SQL 语法必须与 Inceptor/Hive 兼容
#- 示例：若要排除 test 数据库，可添加条件：AND database_name not in ('system', 'test')
#- 示例：若要仅包含指定数据库，可使用条件：AND database_name in ('db1', 'db2')
table.query.sql=SELECT CONCAT(database_name,'.',table_name) \
  FROM system.tables_v \
  WHERE table_format NOT IN ('io.transwarp.scope.ScopeInputFormat','io.transwarp.hyper2drive.HyperdriveInputFormat','hbase','hyperdrive','org.apache.hadoop.hive.ql.io.tdt.refactor.JDBCDBInputFormat') \
    AND database_name not in ('system')

#====================== 数据表来源配置 ======================
#数据表来源模式：sql（从数据库查询，默认模式）或 file（从文件读取）
#sql 模式：使用 table.query.sql 配置的语句从 system.tables_v 中查询数据表
#file 模式：从 table.list.file 配置的文件中读取数据表列表
table.source.mode=file

#====================== 数据表清单文件配置 ======================
#数据表清单文件路径（仅在 table.source.mode = file 时生效）
#文件格式：每行一个数据表，格式为：数据库名。表名
#支持空行和注释行（以 # 开头的行）
#示例：
## 这是一条注释
#default.table1
#default.table2
#prod.user_table
table.list.file=conf/tables.txt

# 运行失败的会将表记录在该文件中
failed.tables.file=conf/failed_tables.txt
